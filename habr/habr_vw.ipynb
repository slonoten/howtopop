{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from math import log, exp, log1p\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import numpy as np\n",
    "import scipy\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "import Stemmer\n",
    "from stop_words import get_stop_words\n",
    "from datetime import datetime\n",
    "import matplotlib\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_post(path):\n",
    "    with open(path) as json_file:\n",
    "        post = json.load(json_file)\n",
    "    hubs = {t['title'] : True for t in post['hubs']}   \n",
    "    return [post['_id'], post['published']['$date'], post['title'], post['author']['url'], \\\n",
    "            post['domain'], hubs, post['content'], post['tags']] \n",
    "\n",
    "def load_posts(path):\n",
    "    for file_name in listdir(path):\n",
    "        file_path = join(path, file_name)\n",
    "        if isfile(file_path):\n",
    "            yield load_post(file_path)\n",
    "            \n",
    "def get_image_count(html):\n",
    "    return len(re.findall('<img.*?>', html))            \n",
    "            \n",
    "def prepare_data(path):\n",
    "    data = pd.DataFrame(load_posts(path), columns = ['_id', 'published', 'title', 'author', 'domain',\\\n",
    "                                                     'hubs', 'content', 'tags'])\n",
    "    data['published'] = pd.to_datetime(data['published'])\n",
    "    # Считаем и нормализуем количество изображений\n",
    "    #data['image_count'] = data['content'].apply(get_image_count)\n",
    "    #data['image_count'] = data['image_count'] / data['image_count'].max()\n",
    "    # Считаем и нормализуем длину текста\n",
    "    data['content_length'] = data['content'].str.len()\n",
    "    data['sites'] = data['content'].apply(\\\n",
    "        lambda html: list(re.findall('<a href=\"https?://(.+?)(?:/.*\"|\")>', html)))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 50s, sys: 17.2 s, total: 2min 7s\n",
      "Wall time: 2min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_train = prepare_data('./train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>domain</th>\n",
       "      <th>hubs</th>\n",
       "      <th>content</th>\n",
       "      <th>tags</th>\n",
       "      <th>content_length</th>\n",
       "      <th>sites</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://habrahabr.ru/company/webnames/blog/121...</td>\n",
       "      <td>2011-06-14 15:52:00</td>\n",
       "      <td>В Турции введена цензура на доменные имена</td>\n",
       "      <td>https://habrahabr.ru/company/webnames/blog/121...</td>\n",
       "      <td>habrahabr.ru</td>\n",
       "      <td>{'Блог компании Webnames.ru': True}</td>\n",
       "      <td>&lt;p&gt;Правительство Турции &lt;/p&gt;запретило доменные...</td>\n",
       "      <td>[]</td>\n",
       "      <td>114</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://geektimes.ru/post/102539/</td>\n",
       "      <td>2010-08-24 17:29:00</td>\n",
       "      <td>Draganflyer X8 — мечта любого шпиона</td>\n",
       "      <td>https://geektimes.ru/users/marks</td>\n",
       "      <td>geektimes.ru</td>\n",
       "      <td>{'Железо': True}</td>\n",
       "      <td>&lt;img src=\"https://habrastorage.org/storage/hab...</td>\n",
       "      <td>[Draganflyer, беспилотники, UAV, шпионство]</td>\n",
       "      <td>2736</td>\n",
       "      <td>{'habrahabr.ru': True, 'gizmodo.com': True}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 _id           published  \\\n",
       "0  https://habrahabr.ru/company/webnames/blog/121... 2011-06-14 15:52:00   \n",
       "1                  https://geektimes.ru/post/102539/ 2010-08-24 17:29:00   \n",
       "\n",
       "                                        title  \\\n",
       "0  В Турции введена цензура на доменные имена   \n",
       "1        Draganflyer X8 — мечта любого шпиона   \n",
       "\n",
       "                                              author        domain  \\\n",
       "0  https://habrahabr.ru/company/webnames/blog/121...  habrahabr.ru   \n",
       "1                   https://geektimes.ru/users/marks  geektimes.ru   \n",
       "\n",
       "                                  hubs  \\\n",
       "0  {'Блог компании Webnames.ru': True}   \n",
       "1                     {'Железо': True}   \n",
       "\n",
       "                                             content  \\\n",
       "0  <p>Правительство Турции </p>запретило доменные...   \n",
       "1  <img src=\"https://habrastorage.org/storage/hab...   \n",
       "\n",
       "                                          tags  content_length  \\\n",
       "0                                           []             114   \n",
       "1  [Draganflyer, беспилотники, UAV, шпионство]            2736   \n",
       "\n",
       "                                         sites  \n",
       "0                                           {}  \n",
       "1  {'habrahabr.ru': True, 'gizmodo.com': True}  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = pd.read_csv('./train_target.csv')\n",
    "train = df_train.merge(target, on = '_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>domain</th>\n",
       "      <th>hubs</th>\n",
       "      <th>content</th>\n",
       "      <th>tags</th>\n",
       "      <th>content_length</th>\n",
       "      <th>sites</th>\n",
       "      <th>favs_lognorm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://habrahabr.ru/company/webnames/blog/121...</td>\n",
       "      <td>2011-06-14 15:52:00</td>\n",
       "      <td>В Турции введена цензура на доменные имена</td>\n",
       "      <td>https://habrahabr.ru/company/webnames/blog/121...</td>\n",
       "      <td>habrahabr.ru</td>\n",
       "      <td>{'Блог компании Webnames.ru': True}</td>\n",
       "      <td>&lt;p&gt;Правительство Турции &lt;/p&gt;запретило доменные...</td>\n",
       "      <td>[]</td>\n",
       "      <td>114</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://geektimes.ru/post/102539/</td>\n",
       "      <td>2010-08-24 17:29:00</td>\n",
       "      <td>Draganflyer X8 — мечта любого шпиона</td>\n",
       "      <td>https://geektimes.ru/users/marks</td>\n",
       "      <td>geektimes.ru</td>\n",
       "      <td>{'Железо': True}</td>\n",
       "      <td>&lt;img src=\"https://habrastorage.org/storage/hab...</td>\n",
       "      <td>[Draganflyer, беспилотники, UAV, шпионство]</td>\n",
       "      <td>2736</td>\n",
       "      <td>{'habrahabr.ru': True, 'gizmodo.com': True}</td>\n",
       "      <td>3.295837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://habrahabr.ru/company/droider/blog/127362/</td>\n",
       "      <td>2011-08-30 16:34:00</td>\n",
       "      <td>Droider Chart. Выпуск 67, прикладной</td>\n",
       "      <td>https://habrahabr.ru/company/droider</td>\n",
       "      <td>habrahabr.ru</td>\n",
       "      <td>{'Блог компании Droider.Ru': True}</td>\n",
       "      <td>Всем, привет!&lt;br&gt;\\r\\n&lt;br&gt;\\r\\nВ новом выпуске &lt;...</td>\n",
       "      <td>[android, android os, Droider Chart, Droider, ...</td>\n",
       "      <td>1420</td>\n",
       "      <td>{'droider.ru': True, 'habrahabr.ru': True}</td>\n",
       "      <td>1.609438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://geektimes.ru/post/119923/</td>\n",
       "      <td>2011-05-25 03:21:00</td>\n",
       "      <td>Играем в ZX Spectrum на iPhone</td>\n",
       "      <td>https://geektimes.ru/users/soulburner</td>\n",
       "      <td>geektimes.ru</td>\n",
       "      <td>{'Смартфоны': True}</td>\n",
       "      <td>&lt;img src=\"https://habrastorage.org/storage/hab...</td>\n",
       "      <td>[zx spectrum, ixpectrum, ios, iphone, zx, ност...</td>\n",
       "      <td>2509</td>\n",
       "      <td>{'zx-spectrum.narod.ru': True, 'www.doggysoft....</td>\n",
       "      <td>2.772589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://geektimes.ru/post/29492/</td>\n",
       "      <td>2008-07-21 14:14:00</td>\n",
       "      <td>Интересное поведение браузеров</td>\n",
       "      <td>https://geektimes.ru/users/tkf</td>\n",
       "      <td>geektimes.ru</td>\n",
       "      <td>{'Чёрная дыра': True}</td>\n",
       "      <td>Заметил интересную вещь. На странице есть див ...</td>\n",
       "      <td>[браузеры, невидимый блок, странность]</td>\n",
       "      <td>343</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 _id           published  \\\n",
       "0  https://habrahabr.ru/company/webnames/blog/121... 2011-06-14 15:52:00   \n",
       "1                  https://geektimes.ru/post/102539/ 2010-08-24 17:29:00   \n",
       "2  https://habrahabr.ru/company/droider/blog/127362/ 2011-08-30 16:34:00   \n",
       "3                  https://geektimes.ru/post/119923/ 2011-05-25 03:21:00   \n",
       "4                   https://geektimes.ru/post/29492/ 2008-07-21 14:14:00   \n",
       "\n",
       "                                        title  \\\n",
       "0  В Турции введена цензура на доменные имена   \n",
       "1        Draganflyer X8 — мечта любого шпиона   \n",
       "2        Droider Chart. Выпуск 67, прикладной   \n",
       "3              Играем в ZX Spectrum на iPhone   \n",
       "4              Интересное поведение браузеров   \n",
       "\n",
       "                                              author        domain  \\\n",
       "0  https://habrahabr.ru/company/webnames/blog/121...  habrahabr.ru   \n",
       "1                   https://geektimes.ru/users/marks  geektimes.ru   \n",
       "2               https://habrahabr.ru/company/droider  habrahabr.ru   \n",
       "3              https://geektimes.ru/users/soulburner  geektimes.ru   \n",
       "4                     https://geektimes.ru/users/tkf  geektimes.ru   \n",
       "\n",
       "                                  hubs  \\\n",
       "0  {'Блог компании Webnames.ru': True}   \n",
       "1                     {'Железо': True}   \n",
       "2   {'Блог компании Droider.Ru': True}   \n",
       "3                  {'Смартфоны': True}   \n",
       "4                {'Чёрная дыра': True}   \n",
       "\n",
       "                                             content  \\\n",
       "0  <p>Правительство Турции </p>запретило доменные...   \n",
       "1  <img src=\"https://habrastorage.org/storage/hab...   \n",
       "2  Всем, привет!<br>\\r\\n<br>\\r\\nВ новом выпуске <...   \n",
       "3  <img src=\"https://habrastorage.org/storage/hab...   \n",
       "4  Заметил интересную вещь. На странице есть див ...   \n",
       "\n",
       "                                                tags  content_length  \\\n",
       "0                                                 []             114   \n",
       "1        [Draganflyer, беспилотники, UAV, шпионство]            2736   \n",
       "2  [android, android os, Droider Chart, Droider, ...            1420   \n",
       "3  [zx spectrum, ixpectrum, ios, iphone, zx, ност...            2509   \n",
       "4             [браузеры, невидимый блок, странность]             343   \n",
       "\n",
       "                                               sites  favs_lognorm  \n",
       "0                                                 {}      0.000000  \n",
       "1        {'habrahabr.ru': True, 'gizmodo.com': True}      3.295837  \n",
       "2         {'droider.ru': True, 'habrahabr.ru': True}      1.609438  \n",
       "3  {'zx-spectrum.narod.ru': True, 'www.doggysoft....      2.772589  \n",
       "4                                                 {}      0.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train[['_id', 'published', 'favs_lognorm']][train['domain'] == 'geektimes.ru'].to_csv('gt_favs.csv', index = False)\n",
    "train[['_id', 'published', 'favs_lognorm']][train['domain'] == 'habrahabr.ru'].to_csv('habr_favs.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "habr_mean_fav = pd.read_csv('habr_favs_mean_pred.csv').fillna(0)\n",
    "gt_mean_fav = pd.read_csv('gt_favs_mean_pred.csv').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gt_mean_fav.columns = habr_mean_fav.columns = ['date', 'favs_mean60', 'favs_mean60_pred']\n",
    "gt_mean_fav['date'] = pd.to_datetime(gt_mean_fav['date'])\n",
    "habr_mean_fav['date'] = pd.to_datetime(habr_mean_fav['date'])\n",
    "gt_mean_fav.set_index('date', inplace = True)\n",
    "habr_mean_fav.set_index('date', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mean_fav(timestamp, domain):\n",
    "    return (habr_mean_fav if domain == 'habrahabr.ru' else gt_mean_fav).loc[timestamp.date(), 'favs_mean60']         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 19s, sys: 0 ns, total: 1min 19s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train['favs_meanlog'] = train.apply(lambda row: log1p(get_mean_fav(row['published'], row['domain'])), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = train.sort_values('published')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_valid = train[train['published'] > '2016-08-31'].count()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_train = train.count()[0] - n_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = train['favs_lognorm'] - train['favs_meanlog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train, data_valid, y_train, y_valid = train[ : n_train], train[n_train : ], y[ : n_train], y[n_train : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_vw_text(text):\n",
    "    text = ' '.join(re.findall('\\w{3,}', text.lower()))\n",
    "    return text.replace('|', ' ').replace(':', ' ').replace('\\n', ' ')\n",
    "\n",
    "def prepare_vw_dict(pref, dic):\n",
    "    return ' '.join('hub_' + t.replace(' ', '_').replace('|', '_').replace(':', '_') for t in dic.keys())\n",
    "\n",
    "def to_vw_format(document, domain, author, tags, label=None):\n",
    "    return (str(label) if label != None else '') + ' |title ' + prepare_vw_text(document.lower())\\\n",
    "        + ' | text ' + prepare_vw_text(text.lower())\\\n",
    "        + ' |cat '+ 'author_' + author.split('/')[-1] + ' domain_' + domain\\\n",
    "        + ' ' + ' '.join('tag_' + tag.replace(' ', '_').replace('|', '_').replace(':', '_')  for tag in tags) + '\\n'\n",
    "            \n",
    "    #+ ' |hub ' + prepare_vw_dict(hubs) + '\\n'\n",
    "\n",
    "def write_data(vw_file, X, y = None):\n",
    "    with open(vw_file, 'w') as out_file:\n",
    "        for i in range(X.shape[0]):\n",
    "            row = X.iloc[i]\n",
    "            out_file.write(to_vw_format(row['title'], row['domain'], row['author'], row['tags'], y.iloc[i] if y is not None else None))\n",
    "                    \n",
    "def train(model_file_name, X, y, passes = 1):\n",
    "    #write_data('vw_train.txt', X, y)\n",
    "    !rm vw_train.txt.cache\n",
    "    vw_call_string = 'vw -d vw_train.txt -c --passes {passes} --holdout_off --loss_function squared --quiet -f {model_file_name}'\\\n",
    "        .format(passes = passes, model_file_name = model_file_name)                \n",
    "    !{vw_call_string}\n",
    "\n",
    "def predict(model_file_name, X):\n",
    "    write_data('vw_test.txt', X)\n",
    "    vw_call_string = 'vw -d vw_test.txt -p predictions.txt -i {model_file_name} --quiet'.format(model_file_name = model_file_name)\n",
    "    !{vw_call_string}\n",
    "    with open('predictions.txt') as pred_file:\n",
    "        test_prediction = [float(label) for label in pred_file.readlines()]\n",
    "    return test_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Чёрная_дыра'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(t.replace(' ', '_') for t in data_train.iloc[1]['hubs'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 |title mail яндекс заключили стратегическое соглашение |cat author_gameboyhippo domain_geektimes.ru tag_поиск tag_реклама tag_технологии tag_контекст tag_партнерство tag_Гришин tag_Mail.Ru\r\n",
      "0.0 |title яндекс поддерживает правильные тарифы интернет |cat author_gameboyhippo domain_geektimes.ru tag_широкополосный_доступ tag_трафик tag_тариф tag_Яндекс tag_Яндекс.Тариф tag_Волож\r\n",
      "0.0 |title яндекс удвоил доходы прибыль |cat author_gameboyhippo domain_geektimes.ru tag_финансы tag_прибыль tag_доход tag_Яндекс tag_реклама tag_статистика tag_Волож\r\n",
      "0.676617878609 |title поиск блогам яндекса вышел беты |cat author_gameboyhippo domain_geektimes.ru tag_блогосфера tag_поиск tag_Яндекс tag_релевантность tag_информация\r\n",
      "-0.0165293019512 |title запущен первый российский поиск wap ресурсам |cat author_gameboyhippo domain_geektimes.ru tag_поиск tag_технологии tag_WAP tag_информация tag_морфология tag_мобильные_устройс\r\n",
      "-0.0165293019512 |title рамблер предлагает сыграть ассоциации |cat author_gameboyhippo domain_geektimes.ru tag_онлайновые_игры tag_поиск tag_ассоциации tag_Rambler_Media tag_Соня_Соколова\r\n",
      "-0.0165293019512 |title мту интел выходит оптовый рынок интернет трафика |cat author_gameboyhippo domain_geektimes.ru tag_широкополосный_доступ tag_трафик tag_МТУ-Интел tag_ADSL tag_Григорий tag_Новицки\r\n",
      "-0.0165293019512 |title яндекс стал украинским сайтом |cat author_gameboyhippo domain_geektimes.ru tag_Яндекс tag_Украина tag_аудитория tag_трафик\r\n",
      "-0.0165293019512 |title яндекс отдаст миллион хорошее образование |cat author_gameboyhippo domain_geektimes.ru tag_поиск tag_информация tag_конкурс tag_Яндекс\r\n",
      "-0.0165293019512 |title яндекс открывает новую почту |cat author_gameboyhippo domain_geektimes.ru tag_почтовые_сервисы tag_интерфейсы tag_веб-службы tag_Яндекс tag_Ajax tag_Павел tag_Зав\r\n"
     ]
    }
   ],
   "source": [
    "!head vw_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = model.txt\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = vw_train.txt.cache\n",
      "Reading datafile = vw_train.txt\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0   0.0000   0.0000       15\n",
      "0.000000 0.000000            2            2.0   0.0000   0.0000       14\n",
      "0.114453 0.228906            4            4.0   0.6766   0.0000       13\n",
      "0.077072 0.039690            8            8.0  -0.0165   0.1359       11\n",
      "0.157051 0.237030           16           16.0   0.5978   0.3062       15\n",
      "0.236589 0.316127           32           32.0  -0.2495   0.2174       16\n",
      "0.239461 0.242332           64           64.0  -0.4906   0.2465       11\n",
      "0.265020 0.290579          128          128.0  -0.7960  -0.4654       14\n",
      "0.303582 0.342145          256          256.0  -1.1097  -0.4652       11\n",
      "0.283059 0.262536          512          512.0  -1.5721  -1.2724       12\n",
      "0.276967 0.270874         1024         1024.0  -0.7232  -1.1571       13\n",
      "0.343126 0.409285         2048         2048.0  -2.8003  -2.5049       12\n",
      "0.422849 0.502572         4096         4096.0  -2.4686  -2.9763        9\n",
      "0.552496 0.682143         8192         8192.0  -2.8620  -3.6142       14\n",
      "0.717680 0.882864        16384        16384.0  -4.6260  -4.0426        9\n",
      "1.025620 1.333561        32768        32768.0  -3.6982  -4.6707       16\n",
      "1.221156 1.416692        65536        65536.0  -5.7372  -3.8125       11\n",
      "1.225924 1.230692       131072       131072.0  -3.3071  -3.8201       14\n",
      "1.041039 0.856153       262144       262144.0  -5.9654  -5.2430       13\n",
      "0.840135 0.639232       524288       524288.0  -5.2982  -3.8615        7\n",
      "0.689715 0.539294      1048576      1048576.0  -4.8320  -4.2935        9\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 166906\n",
      "passes used = 10\n",
      "weighted example sum = 1669060.000000\n",
      "weighted label sum = -6748315.552869\n",
      "average loss = 0.597107\n",
      "best constant = -4.043183\n",
      "total feature number = 21360810\n",
      "CPU times: user 56 ms, sys: 88 ms, total: 144 ms\n",
      "Wall time: 2.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train('model.txt', data_train, y_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = predict('model.txt', data_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.3656119976465866,\n",
       " 1.5873775203763041,\n",
       " 1.3312795801972566,\n",
       " 1.483228879487787,\n",
       " 1.5181015195777448,\n",
       " 1.4618247892349807,\n",
       " 2.0886354946952701,\n",
       " 1.1463478607210433,\n",
       " 2.0615729905827669,\n",
       " 1.5458122328865804,\n",
       " 1.7382605304748624,\n",
       " 3.7049661815041017,\n",
       " 1.8783399842898496,\n",
       " 1.6584789095510417,\n",
       " 2.0615729905827669,\n",
       " 1.7796259512025685,\n",
       " 3.1462552685258953,\n",
       " 1.0128461776861499,\n",
       " 1.0128461776861499]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_history.append(mean_squared_error(y_valid, predictions))\n",
    "mse_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass {} error {} 1 0.613485102316\n",
      "pass {} error {} 6 0.869296151741\n",
      "pass {} error {} 11 1.04439553241\n",
      "pass {} error {} 16 1.18460633737\n",
      "pass {} error {} 21 1.30335176384\n",
      "pass {} error {} 26 1.40656450594\n"
     ]
    }
   ],
   "source": [
    "for passes in range(1, 30, 5):\n",
    "    train('model.txt', data_train, y_train, passes)\n",
    "    predictions = predict('model.txt', data_valid)\n",
    "    print('pass {} error {}', passes, mean_squared_error(y_valid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    title_tfidf = TfidfVectorizer(stop_words=get_stop_words('russian'), analyzer='word', ngram_range=(1, 2))\n",
    "    X_train_title = title_tfidf.fit_transform(data_train['title'])\n",
    "    X_valid_title = title_tfidf.transform(data_valid['title'])\n",
    "    hub_vect = DictVectorizer()\n",
    "    X_train_hub = hub_vect.fit_transform(data_train['hubs'])\n",
    "    X_valid_hub = hub_vect.transform(data_valid['hubs'])\n",
    "    other_dict = DictVectorizer()\n",
    "    X_train_other = other_dict.fit_transform(data_train[['author', 'domain']].T.to_dict().values())\n",
    "    X_valid_other = other_dict.transform(data_valid[['author', 'domain']].T.to_dict().values())\n",
    "    #publ_hour = DictVectorizer()\n",
    "    #X_train_hour = publ_hour.fit_transform([{time.hour:True} for time in data_train['published']])\n",
    "    #X_valid_hour = publ_hour.transform([{time.hour:True} for time in data_valid['published']])\n",
    "    publ_weekday = DictVectorizer()\n",
    "    X_train_weekday = publ_weekday.fit_transform([{time.weekday():True} for time in data_train['published']])\n",
    "    X_valid_weekday = publ_weekday.transform([{time.weekday():True} for time in data_valid['published']])\n",
    "    tags = DictVectorizer()\n",
    "    X_train_tags = tags.fit_transform([dict((t, True) for t in tags) for tags in data_train['tags']])\n",
    "    X_valid_tags = tags.transform([dict((t, True) for t in tags) for tags in data_valid['tags']])\n",
    "    html_tag_regexp = re.compile('<.*?>')\n",
    "    #content_tfidf = HashingVectorizer(stop_words=get_stop_words('russian'), ngram_range=(1, 2), n_features = 2**18)\n",
    "    default_prerpocessor = TfidfVectorizer().build_preprocessor()\n",
    "    remove_html_tags_preprocessor = lambda s: default_prerpocessor(html_tag_regexp.sub('', s))\n",
    "    content_tfidf = TfidfVectorizer(stop_words=get_stop_words('russian'), analyzer='word', ngram_range=(1, 1),\\\n",
    "                                   preprocessor = remove_html_tags_preprocessor)\n",
    "    X_train_content = content_tfidf.fit_transform(data_train['content'])\n",
    "    X_valid_content = content_tfidf.transform(data_valid['content'])\n",
    "    max_len_log = log(data_train['content_length'].max())\n",
    "    X_train_textlen = coo_matrix(data_train['content_length'].apply(lambda x: log1p(x) / max_len_log)).T\n",
    "    X_valid_textlen = coo_matrix(data_valid['content_length'].apply(lambda x: log1p(x) / max_len_log)).T\n",
    "    sites = DictVectorizer()\n",
    "    X_train_sites = sites.fit_transform(data_train['sites'])\n",
    "    X_valid_sites = sites.transform(data_valid['sites'])\n",
    "    X_train = scipy.sparse.hstack([X_train_title, X_train_hub, X_train_other, X_train_content,\\\n",
    "                               X_train_weekday, X_train_tags, X_train_textlen, X_train_sites]).tocsr(copy = False) \n",
    "    X_valid = scipy.sparse.hstack([X_valid_title, X_valid_hub, X_valid_other, X_valid_content,\\\n",
    "                               X_valid_weekday, X_valid_tags, X_valid_textlen, X_valid_sites]).tocsr(copy = False) \n",
    "    return X_train, X_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 56s, sys: 5.18 s, total: 4min 1s\n",
      "Wall time: 4min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, X_valid = extract_features(data_train, data_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузили и обработали данные, попробуем обучить модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 59s, sys: 72 ms, total: 3min 59s\n",
      "Wall time: 3min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reg = linear_model.SGDRegressor(n_iter = 200,  penalty = 'elasticnet', loss = 'squared_epsilon_insensitive', alpha = 0.000001)\n",
    "reg.fit(X_train, y_train)\n",
    "y_valid_pred = reg.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.50402791238449729, 0.54674477247018693]\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_valid, y_valid_pred)\n",
    "mse_history.append(mse)\n",
    "print(mse_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Получим результаты на тесте.\n",
    "Приготовим данные для обучения модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = prepare_data('./test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, X_test = extract_features(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rgs = linear_model.SGDRegressor(n_iter = 100,  penalty = 'elasticnet', loss = 'squared_epsilon_insensitive', alpha = 0.00001)\n",
    "rgs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test_pred = rgs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "habr_mean_fav_last = habr_mean_fav.loc['2016-10-31':]['favs_mean60'].mean()\n",
    "gt_mean_fav_last = gt_mean_fav.loc['2016-10-31':]['favs_mean60'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gt_mean_fav_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_pred_mean_fav(timestamp, domain):\n",
    "    return (habr_mean_fav if domain == 'habrahabr.ru' else gt_mean_fav).loc[ts.date(), 'favs_mean60_pred'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['favs_meanlog'] = test.apply(lambda row: log1p(get_pred_mean_fav(row['published'], row['domain'])), axis = 1)\n",
    "test['favs_lognorm'] = y_test_pred + test['favs_meanlog']\n",
    "test[['_id', 'favs_lognorm']].to_csv(\"my_submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tkf'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'https://geektimes.ru/users/tkf'.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
