{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from math import log, exp\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.linalg import logm, expm\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "import Stemmer\n",
    "from stop_words import get_stop_words\n",
    "from datetime import datetime\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_post(path):\n",
    "    with open(path) as json_file:\n",
    "        post = json.load(json_file)\n",
    "    hubs = {t['title'] : True for t in post['hubs']}   \n",
    "    return [post['_id'], post['published']['$date'], post['title'], post['author']['url'], \\\n",
    "            post['domain'], hubs, post['content'], post['tags']] \n",
    "\n",
    "def load_posts(path):\n",
    "    for file_name in listdir(path):\n",
    "        file_path = join(path, file_name)\n",
    "        if isfile(file_path):\n",
    "            yield load_post(file_path)\n",
    "            \n",
    "def get_image_count(html):\n",
    "    return len(re.findall('<img.*?>', html))            \n",
    "            \n",
    "def prepare_data(path):\n",
    "    data = pd.DataFrame(load_posts(path), columns = ['_id', 'published', 'title', 'author', 'domain',\\\n",
    "                                                     'hubs', 'content', 'tags'])\n",
    "    data['published'] = pd.to_datetime(data['published'])\n",
    "    # Считаем и нормализуем количество изображений\n",
    "    #data['image_count'] = data['content'].apply(get_image_count)\n",
    "    #data['image_count'] = data['image_count'] / data['image_count'].max()\n",
    "    # Считаем и нормализуем длину текста\n",
    "    data['content_length'] = data['content'].str.len()\n",
    "    data['sites'] = data['content'].apply(\\\n",
    "        lambda html: { s:True for s in re.findall('<a href=\"https?://(.+?)(?:/.*\"|\")>', html)})\n",
    "    return data\n",
    "\n",
    "russian_stemmer = Stemmer.Stemmer('ru')\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: russian_stemmer.stemWords(analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 55s, sys: 18.2 s, total: 2min 13s\n",
      "Wall time: 2min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_train = prepare_data('./train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>domain</th>\n",
       "      <th>hubs</th>\n",
       "      <th>content</th>\n",
       "      <th>tags</th>\n",
       "      <th>content_length</th>\n",
       "      <th>sites</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://habrahabr.ru/company/webnames/blog/121...</td>\n",
       "      <td>2011-06-14 15:52:00</td>\n",
       "      <td>В Турции введена цензура на доменные имена</td>\n",
       "      <td>https://habrahabr.ru/company/webnames/blog/121...</td>\n",
       "      <td>habrahabr.ru</td>\n",
       "      <td>{'Блог компании Webnames.ru': True}</td>\n",
       "      <td>&lt;p&gt;Правительство Турции &lt;/p&gt;запретило доменные...</td>\n",
       "      <td>[]</td>\n",
       "      <td>114</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://geektimes.ru/post/102539/</td>\n",
       "      <td>2010-08-24 17:29:00</td>\n",
       "      <td>Draganflyer X8 — мечта любого шпиона</td>\n",
       "      <td>https://geektimes.ru/users/marks</td>\n",
       "      <td>geektimes.ru</td>\n",
       "      <td>{'Железо': True}</td>\n",
       "      <td>&lt;img src=\"https://habrastorage.org/storage/hab...</td>\n",
       "      <td>[Draganflyer, беспилотники, UAV, шпионство]</td>\n",
       "      <td>2736</td>\n",
       "      <td>{'habrahabr.ru': True, 'gizmodo.com': True}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 _id           published  \\\n",
       "0  https://habrahabr.ru/company/webnames/blog/121... 2011-06-14 15:52:00   \n",
       "1                  https://geektimes.ru/post/102539/ 2010-08-24 17:29:00   \n",
       "\n",
       "                                        title  \\\n",
       "0  В Турции введена цензура на доменные имена   \n",
       "1        Draganflyer X8 — мечта любого шпиона   \n",
       "\n",
       "                                              author        domain  \\\n",
       "0  https://habrahabr.ru/company/webnames/blog/121...  habrahabr.ru   \n",
       "1                   https://geektimes.ru/users/marks  geektimes.ru   \n",
       "\n",
       "                                  hubs  \\\n",
       "0  {'Блог компании Webnames.ru': True}   \n",
       "1                     {'Железо': True}   \n",
       "\n",
       "                                             content  \\\n",
       "0  <p>Правительство Турции </p>запретило доменные...   \n",
       "1  <img src=\"https://habrastorage.org/storage/hab...   \n",
       "\n",
       "                                          tags  content_length  \\\n",
       "0                                           []             114   \n",
       "1  [Draganflyer, беспилотники, UAV, шпионство]            2736   \n",
       "\n",
       "                                         sites  \n",
       "0                                           {}  \n",
       "1  {'habrahabr.ru': True, 'gizmodo.com': True}  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = pd.read_csv('./train_target.csv')\n",
    "train = df_train.merge(target, on = '_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_month_mean_favs(date, df, domain, delta = '60d'): \n",
    "    return df[(df['published'] > date - pd.Timedelta(delta)) &\\\n",
    "              (df['published'] < date + pd.Timedelta(delta)) &\\\n",
    "              (df['domain'] == domain)]['favs_lognorm'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['mean_gt_favs'] = train['published'].apply(lambda d: calc_month_mean_favs(d, train, 'geektimes.ru'))\n",
    "train['mean_habr_favs'] = train['published'].apply(lambda d: calc_month_mean_favs(d, train, 'habrahabr.ru'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = train['favs_lognorm'] - train['mean_favs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train, data_valid, y_train, y_valid = train_test_split(train, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_tfidf = TfidfVectorizer(stop_words=get_stop_words('russian'), analyzer='word', ngram_range=(1, 2))\n",
    "X_train_title = title_tfidf.fit_transform(data_train['title'])\n",
    "X_valid_title = title_tfidf.transform(data_valid['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hub_vect = DictVectorizer()\n",
    "X_train_hub = hub_vect.fit_transform(data_train['hubs'])\n",
    "X_valid_hub = hub_vect.transform(data_valid['hubs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "other_dict = DictVectorizer()\n",
    "X_train_other = other_dict.fit_transform(data_train[['author', 'domain']].T.to_dict().values())\n",
    "X_valid_other = other_dict.transform(data_valid[['author', 'domain']].T.to_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "publ_hour = DictVectorizer()\n",
    "X_train_hour = publ_hour.fit_transform([{time.hour:True} for time in data_train['published']])\n",
    "X_valid_hour = publ_hour.transform([{time.hour:True} for time in data_valid['published']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "publ_weekday = DictVectorizer()\n",
    "X_train_weekday = publ_weekday.fit_transform([{time.weekday():True} for time in data_train['published']])\n",
    "X_valid_weekday = publ_weekday.transform([{time.weekday():True} for time in data_valid['published']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags = DictVectorizer()\n",
    "X_train_tags = tags.fit_transform([dict((t, True) for t in tags) for tags in data_train['tags']])\n",
    "X_valid_tags = tags.transform([dict((t, True) for t in tags) for tags in data_valid['tags']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 59s, sys: 1.61 s, total: 3min\n",
      "Wall time: 3min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "html_tag_regexp = re.compile('<.*?>')\n",
    "#content_tfidf = HashingVectorizer(stop_words=get_stop_words('russian'), ngram_range=(1, 2), n_features = 2**18)\n",
    "default_prerpocessor = TfidfVectorizer().build_preprocessor()\n",
    "remove_html_tags_preprocessor = lambda s: default_prerpocessor(html_tag_regexp.sub('', s))\n",
    "content_tfidf = TfidfVectorizer(stop_words=get_stop_words('russian'), analyzer='word', ngram_range=(1, 1),\\\n",
    "                               preprocessor = remove_html_tags_preprocessor)\n",
    "X_train_content = content_tfidf.fit_transform(data_train['content'])\n",
    "X_valid_content = content_tfidf.transform(data_valid['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_len = data_train['content_length'].max()\n",
    "X_train_textlen = coo_matrix(data_train['content_length'] / max_len).T\n",
    "X_valid_textlen = coo_matrix(data_valid['content_length'] / max_len).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sites = DictVectorizer()\n",
    "X_train_sites = sites.fit_transform(data_train['sites'])\n",
    "X_valid_sites = sites.transform(data_valid['sites'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = scipy.sparse.hstack([X_train_title, X_train_hub, X_train_other, X_train_content,\\\n",
    "                               X_train_weekday, X_train_tags, X_train_textlen, X_train_sites]).tocsr(copy = False) #X_train_content,\\\n",
    "X_valid = scipy.sparse.hstack([X_valid_title, X_valid_hub, X_valid_other, X_valid_content,\\\n",
    "                               X_valid_weekday, X_valid_tags, X_valid_textlen, X_valid_sites]).tocsr(copy = False) #X_valid_content,\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузили и обработали данные, попробуем обучить модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 4s, sys: 756 ms, total: 2min 5s\n",
      "Wall time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reg = linear_model.SGDRegressor(n_iter = 100,  penalty = 'elasticnet', loss = 'squared_epsilon_insensitive', alpha = 0.000001)\n",
    "reg.fit(X_train, y_train)\n",
    "y_valid_pred = reg.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.83687382205867233]\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_valid, y_valid_pred)\n",
    "mse_history.append(mse)\n",
    "print(mse_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Получим результаты на тесте.\n",
    "Приготовим данные для обучения модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_tfidf_ = TfidfVectorizer(stop_words=get_stop_words('russian'), analyzer='word', ngram_range=(1, 2))\n",
    "X_title = title_tfidf_.fit_transform(train['title'])\n",
    "hub_vect_ = DictVectorizer()\n",
    "X_hub = hub_vect_.fit_transform(train['hubs'])\n",
    "other_dict_ = DictVectorizer()\n",
    "X_other = other_dict_.fit_transform(train[['author', 'domain']].T.to_dict().values())\n",
    "tags_ = DictVectorizer()\n",
    "X_tags = tags_.fit_transform([dict((t, True) for t in tags) for tags in train['tags']])\n",
    "content_tfidf_ = TfidfVectorizer(stop_words=get_stop_words('russian'), analyzer='word', ngram_range=(1, 1),\\\n",
    "                               preprocessor = remove_html_tags_preprocessor)\n",
    "X_content = content_tfidf_.fit_transform(train['content'])\n",
    "publ_weekday_ = DictVectorizer()\n",
    "X_weekday = publ_weekday.fit_transform([{time.weekday():True} for time in train['published']])\n",
    "max_len_ = train['content_length'].max()\n",
    "X_textlen = coo_matrix(train['content_length'] / max_len_).T\n",
    "X = scipy.sparse.hstack([X_title, X_hub, X_other, X_content, X_weekday, X_tags, X_textlen]).tocsr(copy = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Обучим модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59028766734424143"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgs = linear_model.SGDRegressor(n_iter = 100,  penalty = 'elasticnet', loss = 'squared_epsilon_insensitive', alpha = 0.000001)\n",
    "rgs.fit(X, y)\n",
    "mean_squared_error(y, rgs.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Получим предсказание на тестовых данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = prepare_data('./test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_title = title_tfidf_.transform(test['title'])\n",
    "X_test_hub = hub_vect_.transform(test['hubs'])\n",
    "X_test_other = other_dict_.transform(test[['author', 'domain']].T.to_dict().values())\n",
    "X_test_tags = tags_.transform([dict((t, True) for t in tags) for tags in test['tags']])\n",
    "X_test_content = content_tfidf_.transform(test['content'])\n",
    "X_test_weekday = publ_weekday_.transform([{time.weekday():True} for time in test['published']])\n",
    "X_test_textlen = coo_matrix(test['content_length'] / max_len_).T\n",
    "X_test = scipy.sparse.hstack([X_test_title, X_test_hub, X_test_other, X_test_content, X_test_weekday,\\\n",
    "                              X_test_tags, X_test_textlen]).tocsr(copy = False)\n",
    "y_test_pred = rgs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test['favs_lognorm'] = y_test_pred\n",
    "test[['_id', 'favs_lognorm']].to_csv(\"my_submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head my_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "html = train['content'].iloc[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "{ s:True for s in re.findall('<a href=\"https?://(.+?)(?:/.*\"|\")>', html)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'<a href=\"https?://(.+)\">'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log(2.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'a b'.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "' '.join(['a', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = RussianStemmer(False)\n",
    "stemmer.stem('клонировать')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.columns = ['_id', 'published', 'title', 'author', 'domain', 'hubs', 'content',\\\n",
    "       'tags', 'text', 'image_count', 'text_length', 'sites', 'favs_lognorm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "math.exp(7.047517)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "re.match('[A-Za-z0-9]{3,}', 'Asdsn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target['favs_lognorm'] = np.expm1(target['favs_lognorm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "2**16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['published'].min(), train['published'].max(), test['published'].min(), test['published'].max() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['month'] = train['published'].apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = DateTime('2016-12-31 22:49:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train['mean_favs'] = [calc_month_mean_favs(d, train) for d in train.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.set_index('published', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vis = train[['published', 'mean_favs']].sort_values('published')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vis.plot(x='published', y = 'mean_favs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
