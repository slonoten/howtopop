{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from math import log, exp, log1p\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.linalg import logm, expm\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "import Stemmer\n",
    "from stop_words import get_stop_words\n",
    "from datetime import datetime\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_post(path):\n",
    "    with open(path) as json_file:\n",
    "        post = json.load(json_file)\n",
    "    hubs = {t['title'] : True for t in post['hubs']}   \n",
    "    return [post['_id'], post['published']['$date'], post['title'], post['author']['url'], \\\n",
    "            post['domain'], hubs, post['content'], post['tags']] \n",
    "\n",
    "def load_posts(path):\n",
    "    for file_name in listdir(path):\n",
    "        file_path = join(path, file_name)\n",
    "        if isfile(file_path):\n",
    "            yield load_post(file_path)\n",
    "            \n",
    "def get_image_count(html):\n",
    "    return len(re.findall('<img.*?>', html))            \n",
    "            \n",
    "def prepare_data(path):\n",
    "    data = pd.DataFrame(load_posts(path), columns = ['_id', 'published', 'title', 'author', 'domain',\\\n",
    "                                                     'hubs', 'content', 'tags'])\n",
    "    data['published'] = pd.to_datetime(data['published'])\n",
    "    # Считаем и нормализуем количество изображений\n",
    "    #data['image_count'] = data['content'].apply(get_image_count)\n",
    "    #data['image_count'] = data['image_count'] / data['image_count'].max()\n",
    "    # Считаем и нормализуем длину текста\n",
    "    data['content_length'] = data['content'].str.len()\n",
    "    data['sites'] = data['content'].apply(\\\n",
    "        lambda html: { s:True for s in re.findall('<a href=\"https?://(.+?)(?:/.*\"|\")>', html)})\n",
    "    return data\n",
    "\n",
    "russian_stemmer = Stemmer.Stemmer('ru')\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: russian_stemmer.stemWords(analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 55s, sys: 19.9 s, total: 2min 15s\n",
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_train = prepare_data('./train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>domain</th>\n",
       "      <th>hubs</th>\n",
       "      <th>content</th>\n",
       "      <th>tags</th>\n",
       "      <th>content_length</th>\n",
       "      <th>sites</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://habrahabr.ru/company/webnames/blog/121...</td>\n",
       "      <td>2011-06-14 15:52:00</td>\n",
       "      <td>В Турции введена цензура на доменные имена</td>\n",
       "      <td>https://habrahabr.ru/company/webnames/blog/121...</td>\n",
       "      <td>habrahabr.ru</td>\n",
       "      <td>{'Блог компании Webnames.ru': True}</td>\n",
       "      <td>&lt;p&gt;Правительство Турции &lt;/p&gt;запретило доменные...</td>\n",
       "      <td>[]</td>\n",
       "      <td>114</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://geektimes.ru/post/102539/</td>\n",
       "      <td>2010-08-24 17:29:00</td>\n",
       "      <td>Draganflyer X8 — мечта любого шпиона</td>\n",
       "      <td>https://geektimes.ru/users/marks</td>\n",
       "      <td>geektimes.ru</td>\n",
       "      <td>{'Железо': True}</td>\n",
       "      <td>&lt;img src=\"https://habrastorage.org/storage/hab...</td>\n",
       "      <td>[Draganflyer, беспилотники, UAV, шпионство]</td>\n",
       "      <td>2736</td>\n",
       "      <td>{'habrahabr.ru': True, 'gizmodo.com': True}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 _id           published  \\\n",
       "0  https://habrahabr.ru/company/webnames/blog/121... 2011-06-14 15:52:00   \n",
       "1                  https://geektimes.ru/post/102539/ 2010-08-24 17:29:00   \n",
       "\n",
       "                                        title  \\\n",
       "0  В Турции введена цензура на доменные имена   \n",
       "1        Draganflyer X8 — мечта любого шпиона   \n",
       "\n",
       "                                              author        domain  \\\n",
       "0  https://habrahabr.ru/company/webnames/blog/121...  habrahabr.ru   \n",
       "1                   https://geektimes.ru/users/marks  geektimes.ru   \n",
       "\n",
       "                                  hubs  \\\n",
       "0  {'Блог компании Webnames.ru': True}   \n",
       "1                     {'Железо': True}   \n",
       "\n",
       "                                             content  \\\n",
       "0  <p>Правительство Турции </p>запретило доменные...   \n",
       "1  <img src=\"https://habrastorage.org/storage/hab...   \n",
       "\n",
       "                                          tags  content_length  \\\n",
       "0                                           []             114   \n",
       "1  [Draganflyer, беспилотники, UAV, шпионство]            2736   \n",
       "\n",
       "                                         sites  \n",
       "0                                           {}  \n",
       "1  {'habrahabr.ru': True, 'gizmodo.com': True}  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = pd.read_csv('./train_target.csv')\n",
    "train = df_train.merge(target, on = '_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>domain</th>\n",
       "      <th>hubs</th>\n",
       "      <th>content</th>\n",
       "      <th>tags</th>\n",
       "      <th>content_length</th>\n",
       "      <th>sites</th>\n",
       "      <th>favs_lognorm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://habrahabr.ru/company/webnames/blog/121...</td>\n",
       "      <td>2011-06-14 15:52:00</td>\n",
       "      <td>В Турции введена цензура на доменные имена</td>\n",
       "      <td>https://habrahabr.ru/company/webnames/blog/121...</td>\n",
       "      <td>habrahabr.ru</td>\n",
       "      <td>{'Блог компании Webnames.ru': True}</td>\n",
       "      <td>&lt;p&gt;Правительство Турции &lt;/p&gt;запретило доменные...</td>\n",
       "      <td>[]</td>\n",
       "      <td>114</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://geektimes.ru/post/102539/</td>\n",
       "      <td>2010-08-24 17:29:00</td>\n",
       "      <td>Draganflyer X8 — мечта любого шпиона</td>\n",
       "      <td>https://geektimes.ru/users/marks</td>\n",
       "      <td>geektimes.ru</td>\n",
       "      <td>{'Железо': True}</td>\n",
       "      <td>&lt;img src=\"https://habrastorage.org/storage/hab...</td>\n",
       "      <td>[Draganflyer, беспилотники, UAV, шпионство]</td>\n",
       "      <td>2736</td>\n",
       "      <td>{'habrahabr.ru': True, 'gizmodo.com': True}</td>\n",
       "      <td>3.295837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://habrahabr.ru/company/droider/blog/127362/</td>\n",
       "      <td>2011-08-30 16:34:00</td>\n",
       "      <td>Droider Chart. Выпуск 67, прикладной</td>\n",
       "      <td>https://habrahabr.ru/company/droider</td>\n",
       "      <td>habrahabr.ru</td>\n",
       "      <td>{'Блог компании Droider.Ru': True}</td>\n",
       "      <td>Всем, привет!&lt;br&gt;\\r\\n&lt;br&gt;\\r\\nВ новом выпуске &lt;...</td>\n",
       "      <td>[android, android os, Droider Chart, Droider, ...</td>\n",
       "      <td>1420</td>\n",
       "      <td>{'droider.ru': True, 'habrahabr.ru': True}</td>\n",
       "      <td>1.609438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://geektimes.ru/post/119923/</td>\n",
       "      <td>2011-05-25 03:21:00</td>\n",
       "      <td>Играем в ZX Spectrum на iPhone</td>\n",
       "      <td>https://geektimes.ru/users/soulburner</td>\n",
       "      <td>geektimes.ru</td>\n",
       "      <td>{'Смартфоны': True}</td>\n",
       "      <td>&lt;img src=\"https://habrastorage.org/storage/hab...</td>\n",
       "      <td>[zx spectrum, ixpectrum, ios, iphone, zx, ност...</td>\n",
       "      <td>2509</td>\n",
       "      <td>{'zx-spectrum.narod.ru': True, 'www.doggysoft....</td>\n",
       "      <td>2.772589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://geektimes.ru/post/29492/</td>\n",
       "      <td>2008-07-21 14:14:00</td>\n",
       "      <td>Интересное поведение браузеров</td>\n",
       "      <td>https://geektimes.ru/users/tkf</td>\n",
       "      <td>geektimes.ru</td>\n",
       "      <td>{'Чёрная дыра': True}</td>\n",
       "      <td>Заметил интересную вещь. На странице есть див ...</td>\n",
       "      <td>[браузеры, невидимый блок, странность]</td>\n",
       "      <td>343</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 _id           published  \\\n",
       "0  https://habrahabr.ru/company/webnames/blog/121... 2011-06-14 15:52:00   \n",
       "1                  https://geektimes.ru/post/102539/ 2010-08-24 17:29:00   \n",
       "2  https://habrahabr.ru/company/droider/blog/127362/ 2011-08-30 16:34:00   \n",
       "3                  https://geektimes.ru/post/119923/ 2011-05-25 03:21:00   \n",
       "4                   https://geektimes.ru/post/29492/ 2008-07-21 14:14:00   \n",
       "\n",
       "                                        title  \\\n",
       "0  В Турции введена цензура на доменные имена   \n",
       "1        Draganflyer X8 — мечта любого шпиона   \n",
       "2        Droider Chart. Выпуск 67, прикладной   \n",
       "3              Играем в ZX Spectrum на iPhone   \n",
       "4              Интересное поведение браузеров   \n",
       "\n",
       "                                              author        domain  \\\n",
       "0  https://habrahabr.ru/company/webnames/blog/121...  habrahabr.ru   \n",
       "1                   https://geektimes.ru/users/marks  geektimes.ru   \n",
       "2               https://habrahabr.ru/company/droider  habrahabr.ru   \n",
       "3              https://geektimes.ru/users/soulburner  geektimes.ru   \n",
       "4                     https://geektimes.ru/users/tkf  geektimes.ru   \n",
       "\n",
       "                                  hubs  \\\n",
       "0  {'Блог компании Webnames.ru': True}   \n",
       "1                     {'Железо': True}   \n",
       "2   {'Блог компании Droider.Ru': True}   \n",
       "3                  {'Смартфоны': True}   \n",
       "4                {'Чёрная дыра': True}   \n",
       "\n",
       "                                             content  \\\n",
       "0  <p>Правительство Турции </p>запретило доменные...   \n",
       "1  <img src=\"https://habrastorage.org/storage/hab...   \n",
       "2  Всем, привет!<br>\\r\\n<br>\\r\\nВ новом выпуске <...   \n",
       "3  <img src=\"https://habrastorage.org/storage/hab...   \n",
       "4  Заметил интересную вещь. На странице есть див ...   \n",
       "\n",
       "                                                tags  content_length  \\\n",
       "0                                                 []             114   \n",
       "1        [Draganflyer, беспилотники, UAV, шпионство]            2736   \n",
       "2  [android, android os, Droider Chart, Droider, ...            1420   \n",
       "3  [zx spectrum, ixpectrum, ios, iphone, zx, ност...            2509   \n",
       "4             [браузеры, невидимый блок, странность]             343   \n",
       "\n",
       "                                               sites  favs_lognorm  \n",
       "0                                                 {}      0.000000  \n",
       "1        {'habrahabr.ru': True, 'gizmodo.com': True}      3.295837  \n",
       "2         {'droider.ru': True, 'habrahabr.ru': True}      1.609438  \n",
       "3  {'zx-spectrum.narod.ru': True, 'www.doggysoft....      2.772589  \n",
       "4                                                 {}      0.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train[['_id', 'published', 'favs_lognorm']][train['domain'] == 'geektimes.ru'].to_csv('gt_favs.csv', index = False)\n",
    "train[['_id', 'published', 'favs_lognorm']][train['domain'] == 'habrahabr.ru'].to_csv('habr_favs.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "habr_mean_fav = pd.read_csv('habr_favs_mean_pred.csv').fillna(0)\n",
    "gt_mean_fav = pd.read_csv('gt_favs_mean_pred.csv').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gt_mean_fav.columns = habr_mean_fav.columns = ['date', 'favs_mean60', 'favs_mean60_pred']\n",
    "gt_mean_fav['date'] = pd.to_datetime(gt_mean_fav['date'])\n",
    "habr_mean_fav['date'] = pd.to_datetime(habr_mean_fav['date'])\n",
    "gt_mean_fav.set_index('date', inplace = True)\n",
    "habr_mean_fav.set_index('date', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mean_fav(timestamp, domain):\n",
    "    return (habr_mean_fav if domain == 'habrahabr.ru' else gt_mean_fav).loc[timestamp.date(), 'favs_mean60']         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 17s, sys: 0 ns, total: 1min 17s\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train['favs_meanlog'] = train.apply(lambda row: log1p(get_mean_fav(row['published'], row['domain'])), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = train.sort_values('published')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_valid = train[train['published'] > '2016-08-31'].count()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_train = train.count()[0] - n_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = train['favs_lognorm'] - train['favs_meanlog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train, data_valid, y_train, y_valid = train[ : n_train], train[n_train : ], y[ : n_train], y[n_train : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(data_train, data_valid):\n",
    "    title_tfidf = TfidfVectorizer(stop_words=get_stop_words('russian'), analyzer='word', ngram_range=(1, 2))\n",
    "    X_train_title = title_tfidf.fit_transform(data_train['title'])\n",
    "    X_valid_title = title_tfidf.transform(data_valid['title'])\n",
    "    hub_vect = DictVectorizer()\n",
    "    X_train_hub = hub_vect.fit_transform(data_train['hubs'])\n",
    "    X_valid_hub = hub_vect.transform(data_valid['hubs'])\n",
    "    other_dict = DictVectorizer()\n",
    "    X_train_other = other_dict.fit_transform(data_train[['author', 'domain']].T.to_dict().values())\n",
    "    X_valid_other = other_dict.transform(data_valid[['author', 'domain']].T.to_dict().values())\n",
    "    #publ_hour = DictVectorizer()\n",
    "    #X_train_hour = publ_hour.fit_transform([{time.hour:True} for time in data_train['published']])\n",
    "    #X_valid_hour = publ_hour.transform([{time.hour:True} for time in data_valid['published']])\n",
    "    publ_weekday = DictVectorizer()\n",
    "    X_train_weekday = publ_weekday.fit_transform([{time.weekday():True} for time in data_train['published']])\n",
    "    X_valid_weekday = publ_weekday.transform([{time.weekday():True} for time in data_valid['published']])\n",
    "    tags = DictVectorizer()\n",
    "    X_train_tags = tags.fit_transform([dict((t, True) for t in tags) for tags in data_train['tags']])\n",
    "    X_valid_tags = tags.transform([dict((t, True) for t in tags) for tags in data_valid['tags']])\n",
    "    html_tag_regexp = re.compile('<.*?>')\n",
    "    #content_tfidf = HashingVectorizer(stop_words=get_stop_words('russian'), ngram_range=(1, 2), n_features = 2**18)\n",
    "    default_prerpocessor = TfidfVectorizer().build_preprocessor()\n",
    "    remove_html_tags_preprocessor = lambda s: default_prerpocessor(html_tag_regexp.sub('', s))\n",
    "    content_tfidf = TfidfVectorizer(stop_words=get_stop_words('russian'), analyzer='word', ngram_range=(1, 1),\\\n",
    "                                   preprocessor = remove_html_tags_preprocessor)\n",
    "    X_train_content = content_tfidf.fit_transform(data_train['content'])\n",
    "    X_valid_content = content_tfidf.transform(data_valid['content'])\n",
    "    max_len_log = log(data_train['content_length'].max())\n",
    "    X_train_textlen = coo_matrix(data_train['content_length'].apply(lambda x: log1p(x) / max_len_log)).T\n",
    "    X_valid_textlen = coo_matrix(data_valid['content_length'].apply(lambda x: log1p(x) / max_len_log)).T\n",
    "    sites = DictVectorizer()\n",
    "    X_train_sites = sites.fit_transform(data_train['sites'])\n",
    "    X_valid_sites = sites.transform(data_valid['sites'])\n",
    "    X_train = scipy.sparse.hstack([X_train_title, X_train_hub, X_train_other, X_train_content,\\\n",
    "                               X_train_weekday, X_train_tags, X_train_textlen, X_train_sites]).tocsr(copy = False) \n",
    "    X_valid = scipy.sparse.hstack([X_valid_title, X_valid_hub, X_valid_other, X_valid_content,\\\n",
    "                               X_valid_weekday, X_valid_tags, X_valid_textlen, X_valid_sites]).tocsr(copy = False) \n",
    "    return X_train, X_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 56s, sys: 5.18 s, total: 4min 1s\n",
      "Wall time: 4min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, X_valid = extract_features(data_train, data_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузили и обработали данные, попробуем обучить модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 59s, sys: 72 ms, total: 3min 59s\n",
      "Wall time: 3min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reg = linear_model.SGDRegressor(n_iter = 200,  penalty = 'elasticnet', loss = 'squared_epsilon_insensitive', alpha = 0.000001)\n",
    "reg.fit(X_train, y_train)\n",
    "y_valid_pred = reg.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.50402791238449729, 0.54674477247018693]\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_valid, y_valid_pred)\n",
    "mse_history.append(mse)\n",
    "print(mse_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Получим результаты на тесте.\n",
    "Приготовим данные для обучения модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = prepare_data('./test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, X_test = extract_features(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rgs = linear_model.SGDRegressor(n_iter = 100,  penalty = 'elasticnet', loss = 'squared_epsilon_insensitive', alpha = 0.00001)\n",
    "rgs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test_pred = rgs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "habr_mean_fav_last = habr_mean_fav.loc['2016-10-31':]['favs_mean60'].mean()\n",
    "gt_mean_fav_last = gt_mean_fav.loc['2016-10-31':]['favs_mean60'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gt_mean_fav_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_pred_mean_fav(timestamp, domain):\n",
    "    return (habr_mean_fav if domain == 'habrahabr.ru' else gt_mean_fav).loc[ts.date(), 'favs_mean60_pred'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['favs_meanlog'] = test.apply(lambda row: log1p(get_pred_mean_fav(row['published'], row['domain'])), axis = 1)\n",
    "test['favs_lognorm'] = y_test_pred + test['favs_meanlog']\n",
    "test[['_id', 'favs_lognorm']].to_csv(\"my_submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
