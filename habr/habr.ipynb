{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from math import log, exp, log1p\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.linalg import logm, expm\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "import Stemmer\n",
    "from stop_words import get_stop_words\n",
    "from datetime import datetime\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_post(path):\n",
    "    with open(path) as json_file:\n",
    "        post = json.load(json_file)\n",
    "    hubs = {t['title'] : True for t in post['hubs']}   \n",
    "    return [post['_id'], post['published']['$date'], post['title'], post['author']['url'], \\\n",
    "            post['domain'], hubs, post['content'], post['tags']] \n",
    "\n",
    "def load_posts(path):\n",
    "    for file_name in listdir(path):\n",
    "        file_path = join(path, file_name)\n",
    "        if isfile(file_path):\n",
    "            yield load_post(file_path)\n",
    "            \n",
    "def get_image_count(html):\n",
    "    return len(re.findall('<img.*?>', html))            \n",
    "            \n",
    "def prepare_data(path):\n",
    "    data = pd.DataFrame(load_posts(path), columns = ['_id', 'published', 'title', 'author', 'domain',\\\n",
    "                                                     'hubs', 'content', 'tags'])\n",
    "    data['published'] = pd.to_datetime(data['published'])\n",
    "    # Считаем и нормализуем количество изображений\n",
    "    #data['image_count'] = data['content'].apply(get_image_count)\n",
    "    #data['image_count'] = data['image_count'] / data['image_count'].max()\n",
    "    # Считаем и нормализуем длину текста\n",
    "    data['content_length'] = data['content'].str.len()\n",
    "    data['sites'] = data['content'].apply(\\\n",
    "        lambda html: { s:True for s in re.findall('<a href=\"https?://(.+?)(?:/.*\"|\")>', html)})\n",
    "    return data\n",
    "\n",
    "russian_stemmer = Stemmer.Stemmer('ru')\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: russian_stemmer.stemWords(analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 54s, sys: 18.4 s, total: 2min 13s\n",
      "Wall time: 2min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_train = prepare_data('./train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>domain</th>\n",
       "      <th>hubs</th>\n",
       "      <th>content</th>\n",
       "      <th>tags</th>\n",
       "      <th>content_length</th>\n",
       "      <th>sites</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://habrahabr.ru/company/webnames/blog/121...</td>\n",
       "      <td>2011-06-14 15:52:00</td>\n",
       "      <td>В Турции введена цензура на доменные имена</td>\n",
       "      <td>https://habrahabr.ru/company/webnames/blog/121...</td>\n",
       "      <td>habrahabr.ru</td>\n",
       "      <td>{'Блог компании Webnames.ru': True}</td>\n",
       "      <td>&lt;p&gt;Правительство Турции &lt;/p&gt;запретило доменные...</td>\n",
       "      <td>[]</td>\n",
       "      <td>114</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://geektimes.ru/post/102539/</td>\n",
       "      <td>2010-08-24 17:29:00</td>\n",
       "      <td>Draganflyer X8 — мечта любого шпиона</td>\n",
       "      <td>https://geektimes.ru/users/marks</td>\n",
       "      <td>geektimes.ru</td>\n",
       "      <td>{'Железо': True}</td>\n",
       "      <td>&lt;img src=\"https://habrastorage.org/storage/hab...</td>\n",
       "      <td>[Draganflyer, беспилотники, UAV, шпионство]</td>\n",
       "      <td>2736</td>\n",
       "      <td>{'habrahabr.ru': True, 'gizmodo.com': True}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 _id           published  \\\n",
       "0  https://habrahabr.ru/company/webnames/blog/121... 2011-06-14 15:52:00   \n",
       "1                  https://geektimes.ru/post/102539/ 2010-08-24 17:29:00   \n",
       "\n",
       "                                        title  \\\n",
       "0  В Турции введена цензура на доменные имена   \n",
       "1        Draganflyer X8 — мечта любого шпиона   \n",
       "\n",
       "                                              author        domain  \\\n",
       "0  https://habrahabr.ru/company/webnames/blog/121...  habrahabr.ru   \n",
       "1                   https://geektimes.ru/users/marks  geektimes.ru   \n",
       "\n",
       "                                  hubs  \\\n",
       "0  {'Блог компании Webnames.ru': True}   \n",
       "1                     {'Железо': True}   \n",
       "\n",
       "                                             content  \\\n",
       "0  <p>Правительство Турции </p>запретило доменные...   \n",
       "1  <img src=\"https://habrastorage.org/storage/hab...   \n",
       "\n",
       "                                          tags  content_length  \\\n",
       "0                                           []             114   \n",
       "1  [Draganflyer, беспилотники, UAV, шпионство]            2736   \n",
       "\n",
       "                                         sites  \n",
       "0                                           {}  \n",
       "1  {'habrahabr.ru': True, 'gizmodo.com': True}  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = pd.read_csv('./train_target.csv')\n",
    "train = df_train.merge(target, on = '_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>domain</th>\n",
       "      <th>hubs</th>\n",
       "      <th>content</th>\n",
       "      <th>tags</th>\n",
       "      <th>content_length</th>\n",
       "      <th>sites</th>\n",
       "      <th>favs_lognorm</th>\n",
       "      <th>favs_meanlog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108649</th>\n",
       "      <td>https://geektimes.ru/post/1453/</td>\n",
       "      <td>2006-01-16 21:02:00</td>\n",
       "      <td>Mail.ru и «Яндекс» заключили стратегическое со...</td>\n",
       "      <td>https://geektimes.ru/users/gameboyhippo</td>\n",
       "      <td>geektimes.ru</td>\n",
       "      <td>{'Чёрная дыра': True}</td>\n",
       "      <td>Два крупнейших портала российского интернета —...</td>\n",
       "      <td>[поиск, реклама, технологии, контекст, партнер...</td>\n",
       "      <td>1450</td>\n",
       "      <td>{'www.mail.ru': True, 'www.google.com': True}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.793826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150143</th>\n",
       "      <td>https://geektimes.ru/post/1455/</td>\n",
       "      <td>2006-01-19 21:11:00</td>\n",
       "      <td>«Яндекс» поддерживает «правильные» тарифы на и...</td>\n",
       "      <td>https://geektimes.ru/users/gameboyhippo</td>\n",
       "      <td>geektimes.ru</td>\n",
       "      <td>{'Чёрная дыра': True}</td>\n",
       "      <td>Сегодня стартует &lt;a href=\"http://tarif.yandex....</td>\n",
       "      <td>[широкополосный доступ, трафик, тариф, Яндекс,...</td>\n",
       "      <td>1253</td>\n",
       "      <td>{'tarif.yandex.ru': True}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.793826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135431</th>\n",
       "      <td>https://geektimes.ru/post/1454/</td>\n",
       "      <td>2006-03-21 21:07:00</td>\n",
       "      <td>«Яндекс» удвоил доходы и прибыль</td>\n",
       "      <td>https://geektimes.ru/users/gameboyhippo</td>\n",
       "      <td>geektimes.ru</td>\n",
       "      <td>{'Чёрная дыра': True}</td>\n",
       "      <td>Чистая прибыль (net income) компании &lt;a href=\"...</td>\n",
       "      <td>[финансы, прибыль, доход, Яндекс, реклама, ста...</td>\n",
       "      <td>1184</td>\n",
       "      <td>{'www.yandex.ru': True}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.793826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38576</th>\n",
       "      <td>https://geektimes.ru/post/1452/</td>\n",
       "      <td>2006-03-22 21:00:00</td>\n",
       "      <td>Поиск по блогам от «Яндекса» вышел из беты</td>\n",
       "      <td>https://geektimes.ru/users/gameboyhippo</td>\n",
       "      <td>geektimes.ru</td>\n",
       "      <td>{'Чёрная дыра': True}</td>\n",
       "      <td>&lt;a href=\"http://www.yandex.ru/\"&gt;«Яндекс»&lt;/a&gt; з...</td>\n",
       "      <td>[блогосфера, поиск, Яндекс, релевантность, инф...</td>\n",
       "      <td>1173</td>\n",
       "      <td>{'www.yandex.ru': True, 'blogs.yandex.ru': Tru...</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>5.793826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127317</th>\n",
       "      <td>https://geektimes.ru/post/1457/</td>\n",
       "      <td>2006-03-30 21:58:00</td>\n",
       "      <td>Запущен первый российский поиск по wap-ресурсам</td>\n",
       "      <td>https://geektimes.ru/users/gameboyhippo</td>\n",
       "      <td>geektimes.ru</td>\n",
       "      <td>{'Чёрная дыра': True}</td>\n",
       "      <td>Компания &lt;a href=\"http://www.mail.ru/\"&gt;Mail.Ru...</td>\n",
       "      <td>[поиск, технологии, WAP, информация, морфологи...</td>\n",
       "      <td>575</td>\n",
       "      <td>{'www.mail.ru': True}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.793826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    _id           published  \\\n",
       "108649  https://geektimes.ru/post/1453/ 2006-01-16 21:02:00   \n",
       "150143  https://geektimes.ru/post/1455/ 2006-01-19 21:11:00   \n",
       "135431  https://geektimes.ru/post/1454/ 2006-03-21 21:07:00   \n",
       "38576   https://geektimes.ru/post/1452/ 2006-03-22 21:00:00   \n",
       "127317  https://geektimes.ru/post/1457/ 2006-03-30 21:58:00   \n",
       "\n",
       "                                                    title  \\\n",
       "108649  Mail.ru и «Яндекс» заключили стратегическое со...   \n",
       "150143  «Яндекс» поддерживает «правильные» тарифы на и...   \n",
       "135431                   «Яндекс» удвоил доходы и прибыль   \n",
       "38576          Поиск по блогам от «Яндекса» вышел из беты   \n",
       "127317    Запущен первый российский поиск по wap-ресурсам   \n",
       "\n",
       "                                         author        domain  \\\n",
       "108649  https://geektimes.ru/users/gameboyhippo  geektimes.ru   \n",
       "150143  https://geektimes.ru/users/gameboyhippo  geektimes.ru   \n",
       "135431  https://geektimes.ru/users/gameboyhippo  geektimes.ru   \n",
       "38576   https://geektimes.ru/users/gameboyhippo  geektimes.ru   \n",
       "127317  https://geektimes.ru/users/gameboyhippo  geektimes.ru   \n",
       "\n",
       "                         hubs  \\\n",
       "108649  {'Чёрная дыра': True}   \n",
       "150143  {'Чёрная дыра': True}   \n",
       "135431  {'Чёрная дыра': True}   \n",
       "38576   {'Чёрная дыра': True}   \n",
       "127317  {'Чёрная дыра': True}   \n",
       "\n",
       "                                                  content  \\\n",
       "108649  Два крупнейших портала российского интернета —...   \n",
       "150143  Сегодня стартует <a href=\"http://tarif.yandex....   \n",
       "135431  Чистая прибыль (net income) компании <a href=\"...   \n",
       "38576   <a href=\"http://www.yandex.ru/\">«Яндекс»</a> з...   \n",
       "127317  Компания <a href=\"http://www.mail.ru/\">Mail.Ru...   \n",
       "\n",
       "                                                     tags  content_length  \\\n",
       "108649  [поиск, реклама, технологии, контекст, партнер...            1450   \n",
       "150143  [широкополосный доступ, трафик, тариф, Яндекс,...            1253   \n",
       "135431  [финансы, прибыль, доход, Яндекс, реклама, ста...            1184   \n",
       "38576   [блогосфера, поиск, Яндекс, релевантность, инф...            1173   \n",
       "127317  [поиск, технологии, WAP, информация, морфологи...             575   \n",
       "\n",
       "                                                    sites  favs_lognorm  \\\n",
       "108649      {'www.mail.ru': True, 'www.google.com': True}      0.000000   \n",
       "150143                          {'tarif.yandex.ru': True}      0.000000   \n",
       "135431                            {'www.yandex.ru': True}      0.000000   \n",
       "38576   {'www.yandex.ru': True, 'blogs.yandex.ru': Tru...      0.693147   \n",
       "127317                              {'www.mail.ru': True}      0.000000   \n",
       "\n",
       "        favs_meanlog  \n",
       "108649      5.793826  \n",
       "150143      5.793826  \n",
       "135431      5.793826  \n",
       "38576       5.793826  \n",
       "127317      5.793826  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train[['_id', 'published', 'favs_lognorm']][train['domain'] == 'geektimes.ru'].to_csv('gt_favs.csv', index = False)\n",
    "train[['_id', 'published', 'favs_lognorm']][train['domain'] == 'habrahabr.ru'].to_csv('habr_favs.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "habr_mean_fav = pd.read_csv('habr_favs_mean_pred.csv').fillna(0)\n",
    "gt_mean_fav = pd.read_csv('gt_favs_mean_pred.csv').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gt_mean_fav.columns = habr_mean_fav.columns = ['date', 'favs_mean60', 'favs_mean60_pred']\n",
    "gt_mean_fav['date'] = pd.to_datetime(gt_mean_fav['date'])\n",
    "habr_mean_fav['date'] = pd.to_datetime(habr_mean_fav['date'])\n",
    "gt_mean_fav.set_index('date', inplace = True)\n",
    "habr_mean_fav.set_index('date', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mean_fav(timestamp, domain):\n",
    "    return (habr_mean_fav if domain == 'habrahabr.ru' else gt_mean_fav).loc[ts.date(), 'favs_mean60']         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 17s, sys: 24 ms, total: 1min 17s\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train['favs_meanlog'] = train.apply(lambda row: log1p(get_mean_fav(row['published'], row['domain'])), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = train.sort_values('published')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_valid = train[train['published'] > '2016-08-31'].count()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_train = train.count()[0] - n_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = train['favs_lognorm'] - train['favs_meanlog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train, data_valid, y_train, y_valid = train[ : n_train], train[n_train : ], y[ : n_train], y[n_train : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(data_train, data_valid):\n",
    "    title_tfidf = TfidfVectorizer(stop_words=get_stop_words('russian'), analyzer='word', ngram_range=(1, 2))\n",
    "    X_train_title = title_tfidf.fit_transform(data_train['title'])\n",
    "    X_valid_title = title_tfidf.transform(data_valid['title'])\n",
    "    hub_vect = DictVectorizer()\n",
    "    X_train_hub = hub_vect.fit_transform(data_train['hubs'])\n",
    "    X_valid_hub = hub_vect.transform(data_valid['hubs'])\n",
    "    other_dict = DictVectorizer()\n",
    "    X_train_other = other_dict.fit_transform(data_train[['author', 'domain']].T.to_dict().values())\n",
    "    X_valid_other = other_dict.transform(data_valid[['author', 'domain']].T.to_dict().values())\n",
    "    #publ_hour = DictVectorizer()\n",
    "    #X_train_hour = publ_hour.fit_transform([{time.hour:True} for time in data_train['published']])\n",
    "    #X_valid_hour = publ_hour.transform([{time.hour:True} for time in data_valid['published']])\n",
    "    publ_weekday = DictVectorizer()\n",
    "    X_train_weekday = publ_weekday.fit_transform([{time.weekday():True} for time in data_train['published']])\n",
    "    X_valid_weekday = publ_weekday.transform([{time.weekday():True} for time in data_valid['published']])\n",
    "    tags = DictVectorizer()\n",
    "    X_train_tags = tags.fit_transform([dict((t, True) for t in tags) for tags in data_train['tags']])\n",
    "    X_valid_tags = tags.transform([dict((t, True) for t in tags) for tags in data_valid['tags']])\n",
    "    html_tag_regexp = re.compile('<.*?>')\n",
    "    #content_tfidf = HashingVectorizer(stop_words=get_stop_words('russian'), ngram_range=(1, 2), n_features = 2**18)\n",
    "    default_prerpocessor = TfidfVectorizer().build_preprocessor()\n",
    "    remove_html_tags_preprocessor = lambda s: default_prerpocessor(html_tag_regexp.sub('', s))\n",
    "    content_tfidf = TfidfVectorizer(stop_words=get_stop_words('russian'), analyzer='word', ngram_range=(1, 1),\\\n",
    "                                   preprocessor = remove_html_tags_preprocessor)\n",
    "    X_train_content = content_tfidf.fit_transform(data_train['content'])\n",
    "    X_valid_content = content_tfidf.transform(data_valid['content'])\n",
    "    max_len_log = log(data_train['content_length'].max())\n",
    "    X_train_textlen = coo_matrix(data_train['content_length'].apply(lambda x: log1p(x) / max_len_log)).T\n",
    "    X_valid_textlen = coo_matrix(data_valid['content_length'].apply(lambda x: log1p(x) / max_len_log)).T\n",
    "    sites = DictVectorizer()\n",
    "    X_train_sites = sites.fit_transform(data_train['sites'])\n",
    "    X_valid_sites = sites.transform(data_valid['sites'])\n",
    "    X_train = scipy.sparse.hstack([X_train_title, X_train_hub, X_train_other, X_train_content,\\\n",
    "                               X_train_weekday, X_train_tags, X_train_textlen, X_train_sites]).tocsr(copy = False) \n",
    "    X_valid = scipy.sparse.hstack([X_valid_title, X_valid_hub, X_valid_other, X_valid_content,\\\n",
    "                               X_valid_weekday, X_valid_tags, X_valid_textlen, X_valid_sites]).tocsr(copy = False) \n",
    "    return X_train, X_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 50s, sys: 6.35 s, total: 3min 56s\n",
      "Wall time: 4min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, X_valid = extract_features(data_train, data_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузили и обработали данные, попробуем обучить модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 9s, sys: 24 ms, total: 2min 9s\n",
      "Wall time: 2min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reg = linear_model.SGDRegressor(n_iter = 100,  penalty = 'elasticnet', loss = 'squared_epsilon_insensitive', alpha = 0.000001)\n",
    "reg.fit(X_train, y_train)\n",
    "y_valid_pred = reg.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62436598687365008]\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_valid, y_valid_pred)\n",
    "mse_history.append(mse)\n",
    "print(mse_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Получим результаты на тесте.\n",
    "Приготовим данные для обучения модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = prepare_data('./test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, X_test = extract_features(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30min 25s, sys: 72 ms, total: 30min 25s\n",
      "Wall time: 30min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rgs = linear_model.SGDRegressor(n_iter = 1000,  penalty = 'elasticnet', loss = 'squared_epsilon_insensitive', alpha = 0.00001)\n",
    "rgs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test_pred = rgs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "habr_mean_fav_last = habr_mean_fav.loc['2016-10-31':]['favs_mean60'].mean()\n",
    "gt_mean_fav_last = gt_mean_fav.loc['2016-10-31':]['favs_mean60'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215.41996336996343"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_mean_fav_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_pred_mean_fav(timestamp, domain):\n",
    "    return (habr_mean_fav if domain == 'habrahabr.ru' else gt_mean_fav).loc[ts.date(), 'favs_mean60_pred'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['favs_meanlog'] = test.apply(lambda row: log1p(get_pred_mean_fav(row['published'], row['domain'])), axis = 1)\n",
    "test['favs_lognorm'] = y_test_pred + test['favs_meanlog']\n",
    "test[['_id', 'favs_lognorm']].to_csv(\"my_submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.8647031 , -2.26892618, -3.43648231, ..., -2.29441615,\n",
       "       -2.66372756, -2.59215377])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Получим предсказание на тестовых данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = prepare_data('./test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_title = title_tfidf_.transform(test['title'])\n",
    "X_test_hub = hub_vect_.transform(test['hubs'])\n",
    "X_test_other = other_dict_.transform(test[['author', 'domain']].T.to_dict().values())\n",
    "X_test_tags = tags_.transform([dict((t, True) for t in tags) for tags in test['tags']])\n",
    "X_test_content = content_tfidf_.transform(test['content'])\n",
    "X_test_weekday = publ_weekday_.transform([{time.weekday():True} for time in test['published']])\n",
    "X_test_textlen = coo_matrix(test['content_length'] / max_len_).T\n",
    "X_test = scipy.sparse.hstack([X_test_title, X_test_hub, X_test_other, X_test_content, X_test_weekday,\\\n",
    "                              X_test_tags, X_test_textlen]).tocsr(copy = False)\n",
    "y_test_pred = rgs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test['favs_lognorm'] = y_test_pred\n",
    "test[['_id', 'favs_lognorm']].to_csv(\"my_submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head my_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "html = train['content'].iloc[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "{ s:True for s in re.findall('<a href=\"https?://(.+?)(?:/.*\"|\")>', html)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'<a href=\"https?://(.+)\">'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log(2.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'a b'.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "' '.join(['a', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = RussianStemmer(False)\n",
    "stemmer.stem('клонировать')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.columns = ['_id', 'published', 'title', 'author', 'domain', 'hubs', 'content',\\\n",
    "       'tags', 'text', 'image_count', 'text_length', 'sites', 'favs_lognorm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "math.exp(7.047517)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "re.match('[A-Za-z0-9]{3,}', 'Asdsn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target['favs_lognorm'] = np.expm1(target['favs_lognorm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "2**16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['published'].min(), train['published'].max(), test['published'].min(), test['published'].max() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['month'] = train['published'].apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = DateTime('2016-12-31 22:49:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train['mean_favs'] = [calc_month_mean_favs(d, train) for d in train.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.set_index('published', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vis = train[['published', 'mean_favs']].sort_values('published')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vis.plot(x='published', y = 'mean_favs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_tfidf_ = TfidfVectorizer(stop_words=get_stop_words('russian'), analyzer='word', ngram_range=(1, 2))\n",
    "X_title = title_tfidf_.fit_transform(train['title'])\n",
    "hub_vect_ = DictVectorizer()\n",
    "X_hub = hub_vect_.fit_transform(train['hubs'])\n",
    "other_dict_ = DictVectorizer()\n",
    "X_other = other_dict_.fit_transform(train[['author', 'domain']].T.to_dict().values())\n",
    "tags_ = DictVectorizer()\n",
    "X_tags = tags_.fit_transform([dict((t, True) for t in tags) for tags in train['tags']])\n",
    "content_tfidf_ = TfidfVectorizer(stop_words=get_stop_words('russian'), analyzer='word', ngram_range=(1, 1),\\\n",
    "                               preprocessor = remove_html_tags_preprocessor)\n",
    "X_content = content_tfidf_.fit_transform(train['content'])\n",
    "publ_weekday_ = DictVectorizer()\n",
    "X_weekday = publ_weekday.fit_transform([{time.weekday():True} for time in train['published']])\n",
    "max_len_ = train['content_length'].max()\n",
    "X_textlen = coo_matrix(train['content_length'] / max_len_).T\n",
    "X = scipy.sparse.hstack([X_title, X_hub, X_other, X_content, X_weekday, X_tags, X_textlen]).tocsr(copy = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
